---
title: "Part 3: Quantification"
format:
  html:
    embed-resources: true
    toc: true
    toc-location: left
    toc-depth: 3
    title-block-banner: "#00A7FF"
css: style.css
editor: visual
bibliography: references.bib
---

# Methods for Quantification

In this tutorial, we will perform quantification using two main methods: FeatureCounts and Kallisto.

## FeatureCounts

```{r}
suppressPackageStartupMessages({
  library(SummarizedExperiment)
  library(pheatmap)
  library(RColorBrewer)
  library(stringr)
  library(plotly)
  library(tidyverse)
})
source("../helpers.R")
```

#### Set-up

First, let's fetch the input bam files for each sample. While we could use the bams we generated in the past exercises, keep in mind that these utilized downsampled fastqs in order to speed up computation. The bams provided here used the full fastq files but were processed in an otherwise identical way.

```{bash}
CURR_DIR=$(pwd)
cd ../..

# Download the data
STAR_OUTDIR=results/2_mapping/STAR_alignment
mkdir -p $STAR_OUTDIR
cd $STAR_OUTDIR
echo "Depositing data in "$STAR_OUTDIR
curl -O https://fgcz-gstore.uzh.ch/public/RNASeqCourse/processed_yeast_STAR_aligned.tar

# Extract data
tar -xvf processed_yeast_STAR_aligned.tar
rm -f processed_yeast_STAR_aligned.tar

# Back home
cd $CURR_DIR
```

Second, we need to subset the annotation (gtf) file to include only protein-coding genes. There are a few reasons why we might rather exclude transcript types such as lncRNA and snRNA.

-   Including these other transcript types increase the number of tests, meaning more false positives, and less power to detect protein-coding genes

-   non-coding RNAs do not have poly-A tails and can't be detected with mRNA sequencing, so their absence might be misinterpreted

-   They are sometimes not well annotated and this can adversely enrichment analysis

In general, a good strategy is to exclude them by default and include them by demand (i.e. if the research question explicitly demands it). Let's do this subsetting now.

```{r}
seqAnno <- getFeatureAnnotation("../../data/supplementary-files/Ensembl_R64_genes/genes_annotation_byTranscript.txt", dataFeatureType="transcript")
transcriptsUse = rownames(seqAnno)[seqAnno$type %in% "protein_coding"]
gtf <- readGff("../../data/supplementary-files/Ensembl_R64_genes/genes.gtf")
transcripts <- getGffAttributeField(gtf$attributes,
                                    field="transcript_id", 
                                    attrsep="; *", valuesep=" ")
transcriptsUse = union(transcriptsUse, setdiff(transcripts, rownames(seqAnno))) ## add those where we have no info, e.g. spikes
gtf = gtf[transcripts %in% transcriptsUse, ]

tmpOutDir <- "../../data/tmp"
# We create the tmp directory if it does not exist
dir.create(tmpOutDir, showWarnings=FALSE, recursive=TRUE)

# Save the subset GTF
gtfFile <- file.path(tmpOutDir, "genes_protein_coding.gtf")
write.table(gtf, gtfFile, quote=FALSE, sep="\t", row.names=FALSE, col.names=FALSE)
```

# Count QC

Now that the we have generated the counts, we can start exploring the data. We could proceed with either the counts generated by Kallisto or featureCounts. Let's load the featureCounts for now.

```{r}
# Define meta dataframe for later use
meta <- data.frame(
  Condition=as.factor(rep(c("Glucose", "GlycEth"), each=4)),
  row.names=c(paste0("G", 1:4), paste0("GE", 1:4))
)

# Define some general-use parameters for use later
countDirectoryToUse <- "../../results/3_quant/featureCounts"
sigThresh <- 10
conditionColours <- scales::hue_pal()(length(unique(meta$Condition)))
names(conditionColours) <- unique(meta$Condition)
sampleColours <- conditionColours[meta$Condition]
```

First, we load the counts and combine them into a single data-frame. Note we do this here manually, but there are methods like the aforementioned tximport which perform this operation in a convenient way, and beyond this, correct for biases in the counts. We will be using this method of import in the section on differential expression analysis.

```{r}
# Find the count files on the system
countPaths <- Sys.glob(file.path(countDirectoryToUse, sprintf("*%s.txt", rownames(meta))))
names(countPaths) <- rownames(meta)

# Load into memory and combine
countList <- lapply(rownames(meta), function(sn) {
  count <- data.table::fread(countPaths[[sn]]) %>% 
    as.data.frame() %>%
    column_to_rownames(var="Identifier") %>%
    dplyr::rename_with(~sn)
  return(count)
})
rawCounts <- bind_cols(countList)

# Save merged data for later use
saveRDS(rawCounts, file=file.path(countDirectoryToUse, "mergedCounts.rds"))
```

## Count Statistics

Let's look at the count statistics first. In the first of two plots, we get the total counts for each sample. In the second, we see how many features, and what fraction of all features, exceed our signal threshold.

```{r}
toPlot <- tibble(
  Sample=colnames(rawCounts),
  `Read Counts`=colSums(rawCounts),
  `Fraction of features above threshold`=colSums(rawCounts > sigThresh),
  Percentages=paste(signif(100 * colSums(rawCounts > sigThresh) / nrow(rawCounts), digits=2), "%")
)
plot_ly(toPlot, x = ~Sample, y = ~`Read Counts`, type = "bar") %>%
  layout(title="Total reads", yaxis = list(title = "Counts [Mio]"))
plot_ly(toPlot, x = ~Sample, y = ~`Fraction of features above threshold`, type="bar",
        text=~Percentages, textposition = 'auto') %>%
  layout(title="Genomic features with reads above threshold", yaxis = list(title = "Count"))
```

## Exercise #2

1.  How do the read counts compare to the initial FastQC report?

2.  How many features (in %) are above threshold? Do you think re-sequencing the samples with relatively low counts will be useful?

::: {.callout-note appearance="simple" collapse="true"}
### Solutions

1.  They are lower. We started with around 10M reads or more for each sample.

2.  All around 85%. Re-sequencing is unlikely to be worth our time, unless we are interested in very lowly-expressed genes. GE3 has almost as many features with a count above threshold as GE4, despite having half as many counts.
:::

## Filtering Counts

Before continuing, we should filter out genes which are lowly expressed or absent, which we do this on a per-group basis (think about why this might be). Next, we generate normalised counts using "Variance Stabilisation" as provided in the DESeq2 package. As per the documentation, this function calculates a variance stabilizing transformation (VST) from the fitted dispersion-mean relation(s) and then transforms the count data (normalized by division by the size factors or normalization factors), yielding a matrix of values which now have constant variance along the range of mean values (homoskedastic)[@DESeq2]. There are other functions we could use, such as `rlog` which is less sensitive to size factors, but VST works well in the general case.

```{r}
# First filter genes which are at expressed below threshold in less than half the samples within a group
isPresent <- rawCounts > sigThresh
isPresentCond <- rowsum(t(isPresent * 1), group=meta$Condition)
isPresentCond <- t(sweep(isPresentCond, 1,
                         table(meta$Condition)[rownames(isPresentCond)], FUN="/")) >= 0.5
isValid <- rowMeans(isPresentCond) >= 0.5
rawCountsFilt <- rawCounts[isValid, ]

# Load data into a DESeq2 dataset so we can use the variance stabilizing function from Deseq2
dds <- DESeq2::DESeqDataSetFromMatrix(countData=rawCountsFilt,
                                      colData=meta,
                                      design=~Condition)
vsd <- DESeq2::vst(dds)

# Extract normalized counts
vsdSE <- SummarizedExperiment::assay(vsd)
```

Let's quickly compare the raw and normalised counts to see how they have changed.

```{r}
head(rawCountsFilt)
head(as.data.frame(vsdSE))
```

## Dimensionality Reduction

We now have a count matrix of with the following dimensions: 8 x 6'600. In other words, we have represented each sample by a vector of 6'600 features. In order adequately QC our samples, one important aspect is determining how similar or dissimilar one vector of genes from a specific sample is that of another. However, considering 6'600 dimensions directly is not something that humans are generally capable of doing. This is where dimensionality reduction comes in. We would ideally like to *reduce* the given *dimensions* (a 6'600-dimensional space), down to a 2- or 3-dimensional space.

Two approaches, which are in fact related, are PCA (*principal component analysis*) and MDS (*multidimensional scaling*).

### PCA

PCA is a linear dimensionality reduction technique which, in essence, tries to find a rotation of the data in order to maximise the variance. This is a common method for dimensionality reduction across many different disciplines within Computer Science and beyond. We will use the built-in R-method `prcomp` below to calculate the principal components and manually calculate the variance explained by each component.

Additionally, we will plot a 'scree' plot, which aims to visualise the variance explained by each principal component.

```{r}
# Run PCA
pcDat  <- prcomp(t(vsdSE), scale. = FALSE)

# Calculate explained variance
varExp <- (100*pcDat$sdev^2)/sum(pcDat$sdev^2)

# Store the explained variance of top 8 PCs
varExp_df <- data.frame(PC= paste0("PC",1:8),
                          varExp=varExp[1:8])

# Scree plot
varExp_df %>%
  ggplot(aes(x=PC,y=varExp, group=1)) +
  geom_point(colour="steelblue", size=4) +
  geom_col(fill="steelblue") +
  geom_line() + 
  theme_bw() + ylim(c(0,100))
```

Let's now plot the first and second principal components in 2-dimensions.

```{r}
plot_ly(as.data.frame(pcDat$x), x=~PC1, y=~PC2, color=meta$Condition, colors="Set1",
        type="scatter", mode="markers") %>%
  layout(title="PCA Plot")
```

### Multi-dimensional scaling (MDS)

Multi-dimensional scaling is another dimensionality reduction which aims to best reconstruct pairwise distances between a set of points given a set of distances. Since in the case of RNA-seq we are given the data vectors directly rather than the distance matrices, methods must calculate the distance matrix first upon which the MDS algorithm is then performed. PCA is used in the process to produce a reduced dimensionality projection from the similarities.

Here, we use limma [@limma] to calculate the MDS and use plotly to visualise the pairwise distances in 3-dimensions.

```{r}
mds <- limma::plotMDS(vsdSE, plot=FALSE)
mdsOut <- mds$eigen.vectors[,1:3]
colnames(mdsOut) <- c("Leading logFC dim1", "Leading logFC dim2", 
                      "Leading logFC dim3")
toPlot <- cbind(meta %>% rownames_to_column("Sample"), mdsOut)
plot_ly(toPlot, x=~`Leading logFC dim1`, y=~`Leading logFC dim2`, z=~`Leading logFC dim3`, color=~Condition, colors="Set1", type='scatter3d', mode='markers+text', text=~Sample, textposition = "top right") %>%
  plotly::layout(title="Classical MDS", scene=list(xaxis=list(title = 'Leading logFC dim1'), yaxis = list(title = 'Leading logFC dim2'), zaxis = list(title = 'Leading logFC dim3')))
```

::: {.callout-note appearance="simple" collapse="true"}
#### PCA vs MDS

The following image and linked StackedOverflow post explains well the difference between the two methods.

[![](https://i.stack.imgur.com/WvnU7.png){fig-alt="Visual explanation of the difference between PCA and MDS" fig-align="center"}](https://stats.stackexchange.com/questions/14002/whats-the-difference-between-principal-component-analysis-and-multidimensional)
:::

### Exercise #2

1.  How many genes are used by the function `plotMDS` as inputs. What kind of genes are these?
2.  Change the above function call so that all genes in the count matrix are used as input.

::: {.callout-tip collapse="true" appearance="simple"}
## Solutions

1.  By default, the top 500 genes are used.

2.  We must change the `top` argument in order to include all necessary genes.

    ```{r, eval=FALSE}
    mds <- limma::plotMDS(vsdSE, top=nrow(vsdSE), plot=FALSE)
    ```
:::

## Correlation Plots

We can also quantify the similarity between samples using correlations. This involves simply computing pairwise correlations between samples using the gene count vectors of each samples as inputs. The following methods this method as a basis.

### By-Sample Dendogram and Correlations

We can use hierarchical clustering using the pearson correlations as inputs. This will group samples together in a hierarchical fashion in a tree-like structure.

```{r}
d <- as.dist(1-cor(vsdSE, use="complete.obs"))
hc <- hclust(d, method="ward.D2")
WGCNA::plotDendroAndColors(hc, sampleColours, autoColorHeight=TRUE, hang = -0.1)
```

### Correlation Heatmap

Similarly, we can call `pheatmap` on the correlations to easily visualize the similarity between samples within and across conditions.

```{r}
# Pearson correlation plot 
pheatmap(
  mat               = cor(vsdSE, use="complete.obs"),
  treeheight_row    = 100,
  treeheight_col    = 100,
  cutree_rows       = 2, 
  cutree_cols       = 2,
  silent            = F,
  annotation_col    = meta,
  annotation_colors = list(Condition = conditionColours),
  color             = brewer.pal(n = 9, name = "Blues"),
  fontsize_row      = 12, 
  fontsize_col      = 12,
  display_numbers   = TRUE,
  fontsize_number   = 12)
```

### Top 2'000 Variable Genes Heatmap

```{r}
# First, we center the matrix
vsdSECentered <- sweep(vsdSE, 1, rowMeans(vsdSE))
# Identify high variance features
topGenes <- rownames(vsdSE)[head(order(rowSds(vsdSE, na.rm=TRUE),
                                            decreasing = TRUE), 2000)]

countsToPlot <- vsdSECentered[topGenes,]

# Clustering of high variance features
hmObj <- pheatmap(countsToPlot, 
         clustering_method="ward.D2",
         scale = "row", cluster_rows = TRUE,
         cluster_cols = TRUE, show_rownames = FALSE,
         cutree_rows = 6, cutree_cols = 2,
         treeheight_row = 50, treeheight_col = 50,
         annotation_col = meta,
         fontsize_row = 8, fontsize_col = 9,
         annotation_legend = TRUE,
         fontsize=8)
hmObj
```

## Exercise #3 (time allowing):

Change the above code such that the Kallisto outputs are read in intead of the FeatureCount count matrices. What differences, if any, do you observe?

::: {.callout-tip appearance="simple"}
You can use `stringr::str_remove` as we did above to transform the name of the transcripts into genes (as we did for the previous exercise).
:::
